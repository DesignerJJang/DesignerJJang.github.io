---
layout: post
title:  "UX디자이너의 AI스피커 기획하기 (2)"
date:   2020-02-07 06:52:44 +0000
category : UX
---

<br/>
첫번째 포스팅에서 프로젝트에서 가장 아쉬웠던 두 가지를 적어보았습니다.  
PC, 모바일과 AI스피커의 '다름' 을 빨리 깨닫고 '다른' 기획 방향으로 프로젝트를 이끌었다면 지금 보다 더 나은 결과를 가져올 수 있지 않았을까 하는 생각이 들었습니다.


그리고 프로젝트 산출물을 다시 보니 **무엇을 어떻게 다르게 해야 했는지** 조금은 알 수 있었습니다. 그중에서 화면설계서, 대화설계서를 하나씩 자세하게 살펴보며 문제의 근원을 찾아보겠습니다.


<br/><br/>
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
**INDEX**

[1. 화면설계서](#chapter-1)  
[2. 대화설계서](#chapter-2)  
[3. 그럼 이제 어떻게 하면 될까?](#chapter-3)  
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
##### *※ 본 포스팅에 첨부된 산출물 스크린샷은 실제 산출물과 비슷한 형식으로 재구성한 파일입니다*


<br/><br/><br/>


<a id="chapter-1"></a>
### **1. 화면설계서**  

***  
<br/>

![9_3](https://user-images.githubusercontent.com/60729752/74907957-de4f7c00-53f7-11ea-8966-14c002b40ce1.png)


스크린이 있는 디바이스다보니 기존 Web/App처럼 화면설계서를 작성했습니다. 자세히 뜯어보지 않아도 초안`v0.1`과 최종안`v1.7`의 차이가 보입니다.  

`v0.1`은 작고 다양한 요소들로 화면이 가득 차 있으면서 너무나 깔끔한 하나의 발화예시와 응답 멘트가 있습니다. 반면에 `v1.7`을 보면 큼지막한 폰트, 심플한 화면에 비해 상대적으로 복잡해진 발화예시와 응답 멘트를 확인할 수 있습니다.  


**화면은 단순해지고 발화는 복잡해진 모습으로 최종안이 바뀐 이유는 CUI/VUI위주로 기획을 보완했기 때문입니다.**  
기획을 진행하면서 아래 세 가지 차이점에 대해 알게 되면서 복잡했던 화면도 조금씩 정리할 수 있었습니다. 일정 서비스를 예시로 하나씩 살펴보겠습니다.   


<br/>
>**Web/App과 AI스피커의 차이점**  
- 처리방식의 차이  
- 적합한 기능의 차이  
- 표현 가능한 범위의 차이

<br/>

![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
- **처리방식의 차이**


![12_1](https://user-images.githubusercontent.com/60729752/74516438-b4a7d800-4f53-11ea-9cb0-1a2aca9d0c71.png)


기존 Web/App에서는 **①필수값을 입력(또는 선택)** 한 후 **②완료 버튼**을 눌러 일정등록을 완료합니다.  
반면, AI스피커에서는 필수값을 묻는 **①질문에 대답**<sub>Slot-filling</sub>만 하면 일정등록이 완료됩니다.


즉, CUI에서는 'Slot-filling'이 GUI에서 '완료 버튼을 누르는 것'과 같은 최종 액션이기 때문에 화면 안에 버튼을 넣을 필요가 없었습니다.  

결론적으로 화면에 액션 버튼은 삭제하고, "완료 버튼 눌러줘", "취소 버튼 눌러줘"와 같은 TUI에 기반한 발화들도 자연스럽게 지양할 수 있었습니다.


<br/>
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
- **적합한 기능의 차이**


![13_4](https://user-images.githubusercontent.com/60729752/74520929-e2911a80-4f5b-11ea-9a4e-7caf7e4bcc15.png)


키패드가 있고 섬세한 터치 및 조작이 가능한 Web/App 화면에서는 긴 문장을 입력하거나 다양한 옵션을 선택하는 데 어려움이 없습니다. 또한 일정을 다시 수정할 때도 추가할 때와 동일한 프로세스로 원하는 것만 골라서 편집할 수 있습니다.


하지만 AI스피커로 대화를 통해 상세 옵션을 선택해야 한다면 아마 필요한 옵션의 수 만큼(혹은 그 이상)의 질문과 답변<sub>Multiple Dialog</sub>이 이루어져야 합니다. 예를 들어 등록된 일정을 일부 수정하려면 **어떤 일정의 어떤 옵션을 어떻게 수정할 것인지**에 대한 확인이 필요하므로 **훨씬 긴 Multiple Dialog**를 피할 수 없을 것입니다.


잘 기획된 CUI가 줄 수 있는 장점은 무엇보다 **쉽고 빠르게 태스크를 완료할 수 있다는 것**입니다. Web/App에서 기본적인 기능이라 생각한 CRUD이기에 그 기능을 똑같이 스피커에서 CUI로 풀어내려고 하니 오히려 더 복잡해져서 없느니만 못한 케이스도 있다는 것을 알게 되었습니다. 사실 그런 케이스들은 사용자도 복잡하고 어렵다고 느끼겠지만, 만드는 기획자 입장에서도 만들기 쉽지 않은 것이 대부분 이었습니다.


모든 기능이 CUI로 푼다고 Shortcut이 되거나 쉬워지는 것은 아니기에 대화로 풀었을 때 시너지를 줄 수 있을 만한 기능 위주로 남기고 복잡한 기능들을 많이 걷어냈습니다.  


처음부터 적합한 Use case만 선정해서 정의했더라면 좀 더 탄탄한 설계를 할 수 있었을 텐데, 만들어놓고 걷어내다 보니 빈틈이 생기는 부분이 많이 생길 수 밖에 없었습니다.

<br/>
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
- **표현 가능한 범위의 차이**  


![14_5](https://user-images.githubusercontent.com/60729752/74821622-3b3f2980-5347-11ea-9bfc-cf7887161a7f.png) 


Web/App은 사용자가 **특정 화면에서 할 수 있는 것들을 모두 보여줍니다.** 적절한 GUI와 TUI를 통해 어떻게 제어할 수 있는지 알려주고 다음 액션을 유도합니다. 사용자 역시 화면 안에 있는 것들은 모두 제어할 할 수 있다고 생각하며, 보이지 않는 것에 대해서는 딱히 고려하지 않습니다. *원하는 것은 화면 안에 대부분 있기 때문에 보이지 않는 것까지 고려할 필요가 없습니다.*


반면 AI스피커는 사용자가 **할 수 있는 것이 무엇인지 모두 알려주지 않습니다.** 특정 상황에서 응답을 쉽게 할 수 있도록 가이드를 줄 수는 있지만, 사용자는 상황에 상관없이 어떤 말이든 할 수 있고 그것을 제한할 수 없기 때문입니다.  
예를 들어 사용자는 일정을 등록하다가도 음악을 재생시키고 내일 날씨가 어떤지 물어보고, 이번 주 일정은 뭐가 있는지 물어볼 수 있습니다.

즉, Web/App에서 [할 수 있는 것들을 화면 안에 보여주는 것]처럼 AI스피커도 똑같이 보여주기에는 그 '할 수 있는 것'들이 너무 많기 때문에 모두 보여줄 수 없는 것입니다.


![14_3](https://user-images.githubusercontent.com/60729752/74822240-2adb7e80-5348-11ea-92f4-b61a46a3e379.png)  


극단적인 예시로 아이폰의 일정 추가와 Siri 호출 화면을 예로 든다면 기존에 봐오던 친절한 왼쪽 화면과는 다소 멀어 보이면서 **사용자 발화에 어떤 제한도 두지 않는** 오른쪽 모습을 볼 수 있습니다.  
이와 동일한 맥락으로 샬롯home의 화면설계서도 점점 더 단순한 모습으로 바뀌게 되었습니다.  


<br/><br/>
<a id="chapter-2"></a>
### **2. 대화설계서**  

***  
<br/>

![15_3](https://user-images.githubusercontent.com/60729752/74908013-050db280-53f8-11ea-949e-c3c0ab6fd401.png)


대화설계서 역시 초안과 최종안의 차이가 보입니다.


가장 큰 차이는 Dialog 개발을 위해 **발화 별 Intent와 Output(Action)** 이 추가되었다는 것입니다.  
그리고 `v0.1`은 발화의 종류가 하나씩밖에 없고 조건도 굉장히 간단하지만 `v3.3`을 보면 발화예시가 다양해지고, 발화별 조건이 훨씬 더 복잡해진 것을 볼 수 있습니다.


프로젝트를 진행하면서 동의어를 포함해서 가능한 발화를 계속해서 보완하면서 학습시켰지만, 후반에 QA(Quality Assurance) 에서 테스트할 때는 정말 생각지도 못한 발화들이 하루가 멀다 하고 끊임없이 계속 추가되었습니다.


<br/>
![15](https://user-images.githubusercontent.com/60729752/73918853-c2cb7800-4905-11ea-82ca-45795fc54793.png){:  width="70%"}


<span style="color:red;">어느 정도 비슷한 발화들은 NLU엔진이 찰떡같이 알아서 처리해주면 좋았을 텐데. 엔진은...(특히 한국어는) 아직 사람의 도움이 많이 필요해 보였습니다.</span>


<br/><br/>
또 하나의 복병은 **발화만큼 늘어나는 Output(Action)에 대한 Pre-condition** 이었습니다. 사용자의 ‘발화’가 액션에 대한 Shortcut이 되다 보니 어떤 상황에서 그 말을 했는지, 그 액션을 수행하기 위한 조건들이 무엇인지 모두 확인해야 했습니다.


예를 들어 음악을 듣고 싶다고 가정해 본다면 Web/App을 사용 중일 땐 아래 그림의 왼쪽 flow대로 행동할 것이고, AI스피커 사용자는 **"노래 틀어줘"** 라는 발화 하나로 음악이 재생되기를 바랄 것입니다. 액션이 Step-by-Step으로 이루어진다면 고려해야 하는 조건들은 한정되어 있을 텐데 Shortcut은 한 번에 고려할 조건들이 많아서 복잡해질 수 밖에 없었습니다.

<br/>
![16](https://user-images.githubusercontent.com/60729752/73918854-c2cb7800-4905-11ea-8cdc-b70815b37fb6.png)  


<br/>
**대화설계를 하면서 다른 어떤 것들보다 이런 Pre-condition 들을 얼마나 많이 예상하고 커버할 수 있느냐가 AI스피커 사용성에 지대한 영향을 미치는 것을 알 수 있었습니다.**  
즉, 사용자를 많이 예상하고 커버할수록  
+ 사용자가 의도하고 예측한 대로 응답할 수 있고
+ 동일한 발화라도 상황에 맞는 output을 줄 수 있습니다.


<br/><br/>
<a id="chapter-3"></a>
### **3. 그럼 이제 어떻게 하면 될까?**  

***  
<br/>
산출물의 초안과 최종안을 비교하니 수많은 시행착오와 끊임없는 수정작업이 다시 생각납니다. 그 혼란속에서 **어떻게 하면  잘 기획할 수 있을까, 과연 무엇이 좋은 CUX/VUX일까** 고민을 많이 했는데 결론적으로 그 방향 자체는 기존의 Web/App과 크게 다르지 않았습니다. 여기에 '대화'나 '목소리'를 통한 감성이 더해진다면 정말 잘 기획된 AI스피커라 할 수 있지 않을까 생각합니다.   


<br/>
![24](https://user-images.githubusercontent.com/60729752/74999116-0f828780-549e-11ea-90fa-cb5485cd871e.png)

![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png) 

![24_1](https://user-images.githubusercontent.com/60729752/74999126-13aea500-549e-11ea-8536-5911f7d6710b.png)  


 <br/>
이렇게 Web/App이나 AI스피커의 좋은 UX는 크게 다르지 않지만, 지금까지 언급했던 대로 **기획과정에서는 분명한 차이가 있었습니다.**  


AI스피커를 기획한다면, 아래 두 가지를 반드시 머릿속에 새겨두고 기획해야 합니다.  


<br/><br/>
#### 1. 화면에서 벗어나 대화에 집중하기
+ **모든 기능을 대화로 풀지 않을 것**  
  CUI에 가장 적합한 기능(대화로 풀었을 때 장점이 확실한 기능) 위주로 *선택과 집중*할 것 
+ **화면은 원활한 대화의 수단으로 사용할 것**  
  친절한 화면은 오히려 자연스러운 대화를 방해할 수 있다는 것을 명심할 것
+ **실제로 많이 대화해 볼 것**  
  Text를 문서로 보는 것과 실제로 해보는 것의 차이는 어마어마함. 문서작업을 할수록 감성을 자극할 Prompt와는 점점 멀어질 수 있다는 것을 염두에 둘 것
   

<br/>
#### 2. 최대한 많이 다양하게 예측하기 
+ **'상황'에서 기획을 시작할 것**  
  기능이나 특정 화면을 기준으로 기획을 시작하면 놓친 케이스가 어마어마할 것
+ **발화는 다다익선**  
  절대 혼자 기획하지 말고 발화의 정제나 조합도 많은 사람에게 최대한 다양한 발화를 확보한 후에 할 것
+ **발화나 조건은 아무리 예측해도 끝이 없다는 것을 알고 있을 것**  
  끊임없는 추가나 수정에 유연한 대응이 가능한 템플릿으로 미리 준비할 것  
  


<br/>

정말 많이 배웠다고 생각했는데 이렇게 글로 적어놓고 보니 `그땐 왜 몰랐을까. 너무 당연한 것들 아닌가.` 라는 생각이 듭니다. 기회가 된다면 AI스피커가 아니더라도 CUI/VUI를 기획할 수 있는 프로젝트를 다시 한번 해보고 싶네요. 

<br/><br/>
<center>== 읽어주셔서 감사합니다 ==</center>
<br/>

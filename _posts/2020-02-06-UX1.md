---
layout : post
title : "AI스피커 PJT회고(1)"  
date : 2020-02-06 16:32:24
category : UX
--- 
<center>
<h5> 2019. 2 - 2019. 12 </h5></center>


<br/>
작년(2019년)은 이직 하자마자 AI 홈 디바이스 구축 프로젝트에 투입되어 정신없는 한 해를 보냈습니다.   
gentle pie의 역할은 공통, 유틸리티 기획 전반과 자연어 엔진(IBM 왓슨) Dialog 구축이었고, 저는 **UX기획자이자 공통 및 유틸리티 기획PL**의 역할을 맡았습니다.  

짧지 않은 기간 동안 예전처럼 'UX기획'을 했다는 거 빼고는 B2C, AI스피커, CUI, VUI 등 모든 것이 새로웠던 프로젝트라서 배운 점도 많고, 아쉬운 점도 ~~(훨씬 더)~~ 많습니다.   

프로젝트를 진행하면서 기억에 남는 것들을 그대로 흘려보내기가 아까워서 하나씩 자세하게 다뤄보려고 합니다.  
이번 포스팅에는 업무 개요와 가장 아쉬웠던 포인트들을, 두 번째 포스팅에는 아쉬움이 남을 수 밖에 없던 근원이 무엇인지 산출물과 함께 정리해 보겠습니다. 


<br/><br/>
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png)  
**INDEX**

[1. 어떤 디바이스 였는가](#chapter-1)  
[2. 어떤 역할이었는가](#chapter-2)  
[3. 무엇이 가장 아쉬운가](#chapter-3)  
![line](https://user-images.githubusercontent.com/60729752/75006525-5fb91400-54b5-11ea-80cf-5e46dced8d65.png) 


<br/><br/><br/>
<a id="chapter-1"></a>
### **1. 어떤 디바이스 였는가**  

*** 

<br/>
![](https://user-images.githubusercontent.com/60729752/74207003-0f75d100-4cc1-11ea-8277-2bb245b054e5.png)  
###### 샬롯home (※ 출처: *[Adaption - CharlotteHome](https://adaption.xyz/charlottehome-ai-device)*)
<br/><br/>

#### **스크린이 있는 AI스피커**  
<br/>
지금은 스크린이 있는 스피커([SKT NUGU nemo](https://www.nugu.co.kr/device/nugu_nemo), [KT 기가지니 테이블 TV](https://gigagenie.kt.com/tatv/main.do))들이 국내에도 상용되고 있지만, 프로젝트를 시작한 2019년 2월 당시에는 해외의 일부 스피커([Amazon Echo-show](https://www.amazon.com/dp/B077SXWSRP?ref=ods_ucc_aucc_bp_nrc_ucc), [Lenove Smart Display](https://www.lenovo.com/kr/ko/tablets/virtual-reality-and-smart-devices/smart-home/smart-home-series/Smart-Display-10/p/ZZISZSDSDX1))에만 스크린이 있었습니다.


스크린 없이 오직 음성으로만 의사소통하는 기존 AI스피커는 사용자가 할 수 있는 기능의 한계가 분명히 존재합니다. 간단한 Single-turn 대화로 조작할 수 있는 유틸리티 기능이 대부분이며, 콘텐츠의 종류도 음성으로 제공하기 적합한 것(예: 음악, 팟캐스트 등)로 제한되어 있습니다.  
또한 스피커가 자체적으로 처리하기 어려운(필수적인 혹은 부가적인) 기능들을 대신하거나 보완해주기 위한 모바일 App(예: [구글home앱](https://play.google.com/store/apps/details?id=com.google.android.apps.chromecast.app&hl=ko))을 반드시 갖고 있습니다.  


**만약 AI스피커에 스크린을 탑재한다면** 이러한 기능 및 콘텐츠의 한계를 보완할 수 있으며, 좀 더 쉽고 자유로운 조작을 가능하게 합니다. 즉, AI스피커의 **스크린**은 유틸리티를 제외한 다양한 도메인을 제공하여 더 넓은 타겟층을 확보할 수 있는 중요한 요소라고 할 수 있습니다.  


제가 기획한 샬롯home 역시 스크린을 통해 기존에 없던 **이커머스 도메인**의 Use cases를 다루고, '음성'을 비롯한 다양한 '시각적인 콘텐츠'를 제공하는 파워풀한 AI스피커였습니다.


![](https://user-images.githubusercontent.com/60729752/74306593-54bffe80-4da6-11ea-912d-6c67daca1e84.png) ![](https://user-images.githubusercontent.com/60729752/74307052-c0569b80-4da7-11ea-82a1-1cc5e3dc081f.png)![](https://user-images.githubusercontent.com/60729752/74307077-d3696b80-4da7-11ea-9d10-f1edab346860.png)
###### 샬롯 home의 이커머스 및 유틸리티 서비스 (※ 출처: *[샬롯 home 홈페이지](http://charlottehome.lotte.com/display/charlotteHome.lotte)*)



<br/><br/><br/><br/>
<a id="chapter-2"></a>
### **2. 어떤 역할이었는가**

*** 

<br/>
#### **공통 정책 & 미디어 서비스 및 유틸리티 기획**  


**음악, 레시피, 팟캐스트**와 같은 미디어 서비스를 비롯해 **일정, 메모, 알람** 등의 유틸리티 전반을 기획했으며 디바이스 계정, 에러 케이스 등 **공통 정책**을 정의했습니다. 


지금까지 주로 Web이나 App화면을 기획해왔기에 **AI스피커를 구축하는 프로젝트는 처음**이었습니다.  
~~(앞으로 나올 많은 아쉬운 점들의 밑밥을 깔아봅니다)~~  


개인적으로 PC, 모바일이 아닌 AI스피커의 UX디자인을 해볼 수 있다는 것이 기대를 정말 많이 했습니다. 하지만 기존의 화면기획과 어떻게, 얼마나 다른지 어떤 산출물이 나와야 하는지 하나하나 알아가는 과정이 필요했습니다.  


*※ 기획 초반 국내 스피커들의 선행자료들을 보며 많은 도움을 받았는데, 그중에 [SK NUGU의 음성서비스 디자인 가이드라인](https://www.slideshare.net/NUGU_developers/t-on-2) 문서가 상세예시와 쉬운 설명으로 되어있어 처음 VUI를 기획하는 Beginner 입장에서 도움이 많이 되었습니다.*  


<br/><br/><br/><br/>
<a id="chapter-3"></a>
### **3. 무엇이 가장 아쉬운가**

***

<br/>
프로젝트가 끝나고 *"만약 이랬더라면, 그때 이렇게 했더라면..."* 하는 후회가 많이 남았습니다. 그중에 가장 아쉬운 두 가지를 고르자면, **스크린**과 **부족했던 경험** 입니다. 하나씩 살펴 보겠습니다.
<br/><br/>
#### **① 스크린**  
(부제: 스크린이 없었더라면 조금 덜 헤맸을까?)


![2](https://user-images.githubusercontent.com/60729752/73918338-cf030580-4904-11ea-9934-3fc05154fd40.png)


스크린의 존재 자체가 문제라기보다는  
기획 초반에 'AI스피커'의 연장선이 아닌 **스크린이 있는 디바이스의 연장선**으로 접근한 것이 문제였습니다.


이해를 돕기 위해 그림으로 정리해보겠습니다.


<br/>
**PC와 스마트폰, AI스피커의 User Interface를 간단하게 그려보면 아래와 같습니다.**  

![](https://user-images.githubusercontent.com/60729752/73918340-cf030580-4904-11ea-980b-789e1b9c8016.png)
<br/>
> **GUI** Graphic User Interface(예: PC)  
**TUI** Touch User Interface(예: 스마트폰)  
**CUI** Conversation User Interface(예: 챗봇)  
**VUI** Voice User Interface(예: AI스피커) 
##### ※위 개념들은 예시처럼 1:1로 매칭되는 개념들은 아닙니다.  AI스피커에는 VUI와 CUI를, 스마트폰은 GUI와 TUI를 같이 고려해야 합니다.





<br/><br/><br/>

**그리고 샬롯home은 AI스피커에 스크린이 추가되었기 때문에 아마 아래와 같은 위치에 있을 것입니다.**  

![4](https://user-images.githubusercontent.com/60729752/73918341-cf9b9c00-4904-11ea-931c-d5240de90e51.png)
##### ※ **그림① : AI스피커 연장선에 있는 샬롯 home**  

<br/><br/>
*하지만 기획 초반, **디바이스에 대한 이해 부족으로 PC, 스마트폰과 동일한 위치에서 음성 명령이 추가된 것으로 생각했습니다.***
<br/><br/>
![5](https://user-images.githubusercontent.com/60729752/73918342-d0343280-4904-11ea-8a6e-cae1b101c9c7.png)  
##### ※ **그림② : PC, 스마트폰 연장선에 있는 샬롯 home**   
<br/>
둘의 차이는  
- 그림① : **Voice Conversation이 메인이면서** 대화를 보완하기 위한 수단으로 스크린을 사용하는 것 

- 그림② : **Touch에 적합한 GUI를 갖고 있으면서** 음성명령으로도 기능 수행할 수 있는 것   


<br/>
이렇게 정리해볼 수 있으며, 후자(②)의 방향으로 초반기획을 하다보니 기획 후반부로 갈수록 끊임없는 수정작업을 피할 수 없었습니다. 프로젝트 중에도 *"내가 지금 AI스피커를 기획하는지, PC의 손쉬운 사용(음성명령)을 기획하고 있는지"* 저 조차도 혼란스러운 와중에 산출물을 보완하고 수정하느라 정신이 없었습니다.


지금 생각해보면 대화를 메인으로 하며 스크린은 보조수단으로 사용하는 ①의 방향으로 가는 것이 AI스피커의 사용성을 높일 수 있는 방향이었는데 초반에 캐치하지 못해서 다른 것들에 고민할 시간을 많이 뺏긴 게 가장 아쉬운 포인트 중 한 가지입니다. 

<br/>
##### *차라리 스크린이 없는 스피커였다면 어땠을까? 이것보단 조금 덜 헤매지 않았을까? (적어도 내 화면설계 본능이 튀어나오는 일은 없지 않았을까)하는 생각을 해봅니다.*
<br/>

***

<br/>
#### **② 경험 부족**  
(부제: 두 번째. 나도 그렇듯 모두가 처음이었다)


소프트웨어뿐만 아니라 하드웨어까지 전부 새로 만들다 보니 정말 다양한 회사가 참여한 큰 프로젝트였습니다. 서버, 프론트 개발자만 해도 최소 60명이 넘는 인력이 투입되었으며 기획에 대한 리뷰나 피드백도 평소에 하던 프로젝트보다 훨씬 더 많이 진행했습니다.  
<br/>
지금까지 다른 프로젝트에서 해왔던 대로 화면설계서를 가지고 개발자와 GUI 디자이너, PM과 화면 리뷰를 여러 차례 진행하며 개발요건과 WBS를 정의했습니다.  
<br/><br/>
***사공이 많아서 일까? 우리는 왜 '산'으로 갔을까?***


위에 언급했듯이 저는 AI스피커 기획이 처음이었고 그 과정에서 약간~~(많은)~~ 혼란을 겪었습니다. 하지만 더 큰 문제는 그 **처음**이 저한테만 해당하는 것이 아니었던 것입니다.


프로젝트에 Web이나 App만 하던 사람들이 대부분이었고 저도 **지금까지 해왔던 대로 화면설계서를 리뷰하다 보니** 어느덧 AI스피커가 아닌 8인치 Tablet PC의 '산' 어딘가에 와있었습니다.


<br/>
![6](https://user-images.githubusercontent.com/60729752/73918344-d0343280-4904-11ea-84af-15b75db7b836.png)


주로 개발자, GUI디자이너와 리뷰를 많이 했는데,  


- **개발자**는 이 `화면`으로 진입하는 버튼은 어떤 `화면`에 있는지, `화면`에서 Touchable 한 요소들은 어떤 것들이 있는지, 터치했을 때 어떤 기능을 하는지를 가장 먼저 짚었습니다.  
- **GUI 디자이너**는 이 `화면`에서 기능에 대한 어포던스는 어디에 어떻게 있는지, `화면`에 메뉴 버튼이나 이전 버튼과 같은 디자인 요소들은 왜 없는지 의문을 가졌습니다.  


<span style="color:red;">사실 모든 것을 화면에 보여주지 않아도 괜찮았습니다.  
뭐든 "말"로하면 되는 AI스피커니까.</span>


하지만 여전히 혼란 속에 있던 저는 리뷰를 거듭할수록 사람들의 의견을 반영하는 데 급급했고, 화면은 점점 미궁 속으로 빠졌습니다.  
돌이켜 보면 나조차도 기획에 대한 확신이 없는데 과연 누구를 설득시킬 수 있었을까 싶습니다.  
결국 프로젝트 후반부로 갈수록 뭔가 잘못되었다는 것을 깨닫고 보완하는데 너무 힘들었던 기억이 나네요.

![21](https://user-images.githubusercontent.com/60729752/74498550-48fa4680-4f24-11ea-86f1-b7ff3b9e8166.png){:  width="50%"}

>나도 그렇듯.너도 처음이구나


그 당시에 내가 혹은 누군가 한 명이라도 가이드를 잡아줄 수 있었다면, 화면이 아닌 대화설계서(CUI) 리뷰를 먼저했다면, 좀 덜 고생하고 좀 더 잘 만들 수 있었을 거라는 아쉬움이 남습니다.

<br/><br/>

이어지는 포스트에는 실제 산출물로 살펴보는 문제의 근원과 해결방법을 정리해보겠습니다.  
[*UX디자이너의 AI스피커 PJT회고(2)*](https://designerjjang.github.io/ux/2020/02/07/UX2.html) 

<br/><br/>

